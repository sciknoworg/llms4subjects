{
    "@graph": [
        {
            "@id": "gnd:1249485533",
            "sameAs": "Rauber, Jonas"
        },
        {
            "@id": "gnd:36187-2",
            "sameAs": "Eberhard Karls Universit\u00e4t T\u00fcbingen"
        },
        {
            "@id": "https://www.tib.eu/de/suchen/id/TIBKAT%3A1783947292",
            "@type": "bibo:Thesis",
            "P1053": "1 Online-Ressource (147 Seiten)",
            "description": "Illustrationen",
            "identifier": [
                "(firstid)KXP:1783947292",
                "(ppn)1783947292",
                "(doi)10.15496/publikation-63213"
            ],
            "subject": [
                "(classificationName=linseach:mapping)inf",
                "Deep learning",
                "(classificationName=ddc)006.31",
                "K\u00fcnstliche Intelligenz",
                "(classificationName=ddc-dbn)004",
                "Maschinelles Lernen",
                "Neuronales Netz"
            ],
            "title": "Advances in reliably evaluating and improving adversarial robustness",
            "abstract": [
                "Machine learning has made enormous progress in the last five to ten years. We can now make a computer, a machine, learn complex perceptual tasks from data rather than explicitly programming it. When we compare modern speech or image recognition systems to those from a decade ago, the advances are awe-inspiring. The susceptibility of machine learning systems to small, maliciously crafted adversarial perturbations is less impressive. Almost imperceptible pixel shifts or background noises can completely derail their performance. While humans are often amused by the stupidity of artificial intelligence, engineers worry about the security and safety of their machine learning applications, and scientists wonder how to make machine learning models more robust and more human-like. This dissertation summarizes and discusses advances in three areas of adversarial robustness. First, we introduce a new type of adversarial attack against machine learning models in real-world black-box scenarios. Unlike previous attacks, it does not require any insider knowledge or special access. Our results demonstrate the concrete threat caused by the current lack of robustness in machine learning applications. Second, we present several contributions to deal with the diverse challenges around evaluating adversarial robustness. The most fundamental challenge is that common attacks cannot distinguish robust models from models with misleading gradients. We help uncover and solve this problem through two new types of attacks immune to gradient masking. Misaligned incentives are another reason for insufficient evaluations. We published joint guidelines and organized an interactive competition to mitigate this problem. Finally, our open-source adversarial attacks library Foolbox empowers countless researchers to overcome common technical obstacles. Since robustness evaluations are inherently unstandardized, straightforward access to various attacks is more than a technical convenience; it promotes thorough evaluations. Third, we showcase a fundamentally new neural network architecture for robust classification. It uses a generative analysis-by-synthesis approach. We demonstrate its robustness using a digit recognition task and simultaneously reveal the limitations of prior work that uses adversarial training. Moreover, further studies have shown that our model best predicts human judgments on so-called controversial stimuli and that our approach scales to more complex datasets.",
                "Machine Learning hat in den letzten f\u00fcnf bis zehn Jahren enorme Fortschritte gemacht. Heutzutage k\u00f6nnen wir Computer, Maschinen, dazu bringen, komplexe Wahrnehmungsaufgaben aus Daten zu lernen, anstatt sie explizit zu programmieren. Besonders moderne Sprach- und Bilderkennungssysteme erreichen im Vergleich zu denen von vor einem Jahrzehnt mittlerweile eine beeindruckende Genauigkeit. Weniger beeindruckend ist die Anf\u00e4lligkeit von Machine-Learning-Systemen f\u00fcr kleine, b\u00f6swillig herbeigef\u00fchrte St\u00f6rungen. Kaum wahrnehmbare Hintergrundger\u00e4usche oder Ver\u00e4nderungen ausgew\u00e4hlter Pixel k\u00f6nnen sie komplett in die Irre f\u00fchren. W\u00e4hrend Menschen sich oft \u00fcber diese Dummheit k\u00fcnstlicher Intelligenz am\u00fcsieren, machen sich Entwickler Sorgen um die Sicherheit ihrer Machine-Learning-Anwendungen, und Wissenschaftler suchen nach robusteren Machine-Learning-Modellen, deren Wahrnehmung mehr der des Menschen entspricht. Diese Dissertation fasst Fortschritte in drei Bereichen rund um die Robustheit gegen gezielte St\u00f6rungen zusammen und diskutiert ihre Implikationen. Erstens stellen wir eine neue Art Attacke vor, die Machine-Learning-Anwendungen ganz unmittelbar angreifen kann. Im Gegensatz zu vorangegangenen Attacken erfordert sie weder Insiderwissen noch besonderen Zugang zum Modell. Unsere Ergebnisse zeigen die konkrete Bedrohung, die durch die derzeitig fehlende Robustheit von Machine-Learning-Anwendungen entsteht. Zweitens pr\u00e4sentieren wir mehrere Arbeiten, die sich mit den verschiedenen Herausforderungen bei der Robustheits-Evaluierung befassen. Die grundlegendste Herausforderung dabei ist, dass g\u00e4ngige Testmethoden robuste Modelle nicht von Modellen mit irref\u00fchrenden Gradienten unterscheiden k\u00f6nnen. Durch zwei neue Arten von Testmethoden, die immun gegen irref\u00fchrende Gradienten sind, helfen wir, dieses Problem aufzudecken und zu l\u00f6sen. Falsche Anreize sind ein weiterer Grund f\u00fcr fehlerhafte Evaluierungen. Um dieses Problem zu lindern, haben wir gemeinsame Richtlinien ver\u00f6ffentlicht und einen interaktiven Wettbewerb organisiert. Schlussendlich haben wir mit Foolbox eine Open-Source-Softwarebibliothek mit Testmethoden ver\u00f6ffentlicht, die unz\u00e4hligen Forschern hilft, g\u00e4ngige technische Hindernisse beim Evaluieren von Modellen zu \u00fcberwinden. Da die Evaluierung von Robustheit grunds\u00e4tzlich nicht standardisiert werden kann, f\u00fchrt der einfache Zugang zu verschiedenen Testmethoden in der Praxis dar\u00fcberhinaus zu gr\u00fcndlicheren Evaluierungen und verl\u00e4sslicheren Ergebnissen. Drittens haben wir eine v\u00f6llig neue neuronale Netzwerk-Architektur entwickelt, die robustes Klassifizieren erm\u00f6glichen soll. Sie verwendet einen generativen Analysis-by-Synthesis-Ansatz. Am Beispiel eines Modells zur Ziffernerkennung demonstrieren wir die Robustheit dieser Architektur und zeigen gleichzeitig die Grenzen fr\u00fcherer Arbeiten auf, die Adversarial Training verwenden. Neuere Studien haben au\u00dferdem gezeigt, dass unser Modell die menschliche Wahrnehmung sogenannter kontroverser Stimuli besser als andere Modelle vorhersagt und dass unser Ansatz auch auf komplexere Datens\u00e4tze skaliert."
            ],
            "contributor": "Technische Informationsbibliothek (TIB)",
            "creator": [
                "gnd:36187-2",
                "gnd:1249485533"
            ],
            "isPartOf": "(collectioncode)GBV-ODiss",
            "issued": "2021",
            "language": "http://id.loc.gov/vocabulary/iso639-1/en",
            "license": "open access",
            "medium": "rda:termList/RDACarrierType/1018",
            "isLike": "doi:10.15496/publikation-63213",
            "P60163": "T\u00fcbingen"
        }
    ],
    "@id": "urn:x-arq:DefaultGraphNode",
    "@context": {
        "sameAs": "http://www.w3.org/2002/07/owl#sameAs",
        "subject": "http://purl.org/dc/elements/1.1/subject",
        "license": "http://purl.org/dc/terms/license",
        "isLike": {
            "@id": "http://umbel.org/umbel#isLike",
            "@type": "@id"
        },
        "contributor": "http://purl.org/dc/terms/contributor",
        "abstract": "http://purl.org/dc/terms/abstract",
        "title": "http://purl.org/dc/elements/1.1/title",
        "isPartOf": "http://purl.org/dc/terms/isPartOf",
        "identifier": "http://purl.org/dc/elements/1.1/identifier",
        "creator": {
            "@id": "http://purl.org/dc/terms/creator",
            "@type": "@id"
        },
        "issued": "http://purl.org/dc/terms/issued",
        "P60163": "http://www.rdaregistry.info/Elements/u/#P60163",
        "description": "http://purl.org/dc/elements/1.1/description",
        "language": {
            "@id": "http://purl.org/dc/terms/language",
            "@type": "@id"
        },
        "medium": {
            "@id": "http://purl.org/dc/terms/medium",
            "@type": "@id"
        },
        "P1053": "http://iflastandards.info/ns/isbd/elements/P1053",
        "umbel": "http://umbel.org/umbel#",
        "rdau": "http://www.rdaregistry.info/Elements/u/#",
        "owl": "http://www.w3.org/2002/07/owl#",
        "dcterms": "http://purl.org/dc/terms/",
        "bibo": "http://purl.org/ontology/bibo/",
        "rdam": "http://www.rdaregistry.info/Elements/m/#",
        "gnd": "http://d-nb.info/gnd/",
        "isbd": "http://iflastandards.info/ns/isbd/elements/",
        "rda": "http://rdvocab.info/",
        "doi": "https://doi.org/"
    }
}