{
    "@graph": [
        {
            "@id": "gnd:1203634315",
            "sameAs": "Herman, Michael"
        },
        {
            "@id": "gnd:4041457-7",
            "sameAs": "Navigation"
        },
        {
            "@id": "gnd:4193754-5",
            "sameAs": "Maschinelles Lernen"
        },
        {
            "@id": "gnd:4261462-4",
            "sameAs": "Robotik"
        },
        {
            "@id": "gnd:4304075-5",
            "sameAs": "Autonomer Roboter"
        },
        {
            "@id": "gnd:4825546-4",
            "sameAs": "Best\u00e4rkendes Lernen (K\u00fcnstliche Intelligenz)"
        },
        {
            "@id": "https://www.tib.eu/de/suchen/id/TIBKAT%3A1688814264",
            "@type": "bibo:Thesis",
            "P1053": "1 Online-Ressource (xii, 122 Seiten)",
            "description": "Illustrationen, Diagramme",
            "identifier": [
                "(contract)FRUB-opus-154618",
                "(doi)10.6094/UNIFR/154618",
                "(firstid)KXP:1688814264",
                "(ppn)1688814264"
            ],
            "http://purl.org/dc/elements/1.1/subject": [
                "(classificationName=linseach:mapping)ver",
                "(classificationName=ddc-dbn)621.3",
                "(classificationName=ddc)629.892",
                "(classificationName=linseach:mapping)elt",
                "(classificationName=linseach:mapping)phy"
            ],
            "title": "Simultaneous estimation of rewards and dynamics in inverse reinforcement learning problems",
            "abstract": [
                "Abstract: As the capabilities of autonomous systems improve, they can solve more and more tasks in increasingly complex environments. Often, the autonomous system needs adjustments to the specific task or environment, which typically requires extensive research and engineering. By allowing non-experts to adjust the systems to new behaviors and goals, they are faster and easier to deploy for various tasks and environments, which increases their acceptability. An intuitive way to describe a task is to provide demonstrations of desired behavior. These demonstrations can be used to learn a representation of the expert's motivation and goal.<br><br>Learning from Demonstration is a class of approaches offering the possibility to teach new behaviors by demonstrating the task instead of programming it directly. Two subfields of Learning from Demonstration are Behavioral Cloning and Inverse Reinforcement Learning (IRL). Approaches from Behavioral Cloning estimate the expert's policy directly from demonstrations and therefore learn to mimic the expert. However, the learned policies are typically only appropriate if the environment, the dynamics, and the task remain unchanged. A popular approach that learns more generalizable representations is Inverse Reinforcement Learning, which estimates the unknown reward function of a Markov Decision Process (MDP) from demonstrations of an expert. Many Inverse Reinforcement Learning approaches exist that solve the problem under various assumptions. Most of them assume the environment's dynamics to be known, that they can be learned from expert demonstrations, that additional samples from suboptimal behavior can be queried, or that appropriate heuristics account for unknown transition models. However, these assumptions are often not satisfied, since transition models of environments can be complex, accurate models may be unknown, querying samples may be too expensive, and heuristics may tamper with the reward estimate.<br><br>To solve IRL problems under unknown dynamics, we propose a framework that simultaneously estimates both the reward function and the dynamics from expert demonstrations. This is possible, as both influence the expert's policy and thus the long-term behavior. Therefore, not only the observed transitions of the expert's demonstrations but also the observed actions contain information about the dynamics of the environment. The contribution of this thesis is the formulation of a new problem class, called Simulta ...",
                "Abstract: Da sich die F\u00e4higkeiten autonomer Systemen stetig verbessern, k\u00f6nnen sie in zunehmend komplexeren Umgebungen immer vielseitigere Aufgaben l\u00f6sen. H\u00e4ufig ist es dabei n\u00f6tig das autonome System an die spezifische Aufgabe oder die jeweilige Umwelt anzupassen, was typischerweise eine umfangreiche Forschung und Entwicklung voraussetzt. Um die Akzeptanz solcher Systeme zu erh\u00f6hen, ist es erforderlich deren Einsatz f\u00fcr verschiedene Aufgaben schnell und einfach anhand von Anpassungen der Verhaltensweisen und Ziele durch Nicht-Experten zu erm\u00f6glichen. Eine intuitive Art und Weise Aufgaben zu beschreiben ist das Bereitstellen von Demonstrationen erw\u00fcnschten Verhaltens. Diese Demonstrationen k\u00f6nnen verwendet werden, um eine Repr\u00e4sentation der Motivation und des Ziels des Experten zu lernen.<br><br>Das Lernen aus Demonstrationen beschreibt eine Klasse von Ans\u00e4tzen, anhand derer neue Verhaltensweisen trainiert werden k\u00f6nnen, indem Funktionen und Aufgaben vorgef\u00fchrt werden, anstatt diese zu programmieren. Zwei Teilbereiche des Lernens aus Demonstrationen sind das Klonen von Verhalten und inverses best\u00e4rkendes Lernen. Ans\u00e4tze aus dem Bereich des Klonens von Verhalten sch\u00e4tzen die Strategie des Experten aus dessen Demonstrationen und lernen somit diesen zu imitieren. Allerdings sind die erlernten Strategien nur geeignet, wenn sich die Umwelt, ihre Dynamik sowie die Aufgabe nicht \u00e4ndern. Ein popul\u00e4rer Ansatz, der generalisierbarere Repr\u00e4sentationen lernt, ist das inverse best\u00e4rkende Lernen beziehungsweise Inverse Reinforcement Learning (IRL). Dabei wird die Belohnungsfunktion eines Markow-Entscheidungsprozesses aus Demonstrationen eines Experten gesch\u00e4tzt, wobei die Belohnungsfunktion als Motivation oder Ziel interpretiert werden kann. Es existiert eine Vielzahl an Ans\u00e4tzen des inversen best\u00e4rkenden Lernens, die das Problem unter unterschiedlichen Annahmen l\u00f6sen. Die meisten dieser Ans\u00e4tze nehmen an, dass ein akkurates Dynamikmodell vorhanden ist, dass das Modell aus Expertendemonstrationen gesch\u00e4tzt werden kann, dass zus\u00e4tzliche Demonstrationen suboptimalen Verhaltens abgefragt werden k\u00f6nnen oder dass Heuristiken verwendet werden, um ein nicht vorhandenes Transitionsmodel zu kompensieren. Allerdings werden viele dieser Annahmen h\u00e4ufig verletzt, weil das Dynamikmodel einer Umwelt sehr komplex sein kann, weil akkurate Modelle h\u00e4ufig nicht vorhanden sind, weil zus\u00e4tzliche Demonstrationen zu teuer sein k ..."
            ],
            "contributor": "Technische Informationsbibliothek (TIB)",
            "creator": "gnd:1203634315",
            "isPartOf": "(collectioncode)GBV-ODiss",
            "issued": "2020",
            "language": "http://id.loc.gov/vocabulary/iso639-1/en",
            "license": "open access",
            "medium": "rda:termList/RDACarrierType/1018",
            "subject": [
                "gnd:4193754-5",
                "gnd:4041457-7",
                "gnd:4825546-4",
                "gnd:4304075-5",
                "gnd:4261462-4"
            ],
            "isLike": "doi:10.6094/UNIFR/154618",
            "P60163": "Freiburg"
        }
    ],
    "@id": "urn:x-arq:DefaultGraphNode",
    "@context": {
        "sameAs": "http://www.w3.org/2002/07/owl#sameAs",
        "identifier": "http://purl.org/dc/elements/1.1/identifier",
        "subject": {
            "@id": "http://purl.org/dc/terms/subject",
            "@type": "@id"
        },
        "P1053": "http://iflastandards.info/ns/isbd/elements/P1053",
        "abstract": "http://purl.org/dc/terms/abstract",
        "language": {
            "@id": "http://purl.org/dc/terms/language",
            "@type": "@id"
        },
        "contributor": "http://purl.org/dc/terms/contributor",
        "creator": {
            "@id": "http://purl.org/dc/terms/creator",
            "@type": "@id"
        },
        "isLike": {
            "@id": "http://umbel.org/umbel#isLike",
            "@type": "@id"
        },
        "title": "http://purl.org/dc/elements/1.1/title",
        "license": "http://purl.org/dc/terms/license",
        "isPartOf": "http://purl.org/dc/terms/isPartOf",
        "P60163": "http://www.rdaregistry.info/Elements/u/#P60163",
        "description": "http://purl.org/dc/elements/1.1/description",
        "issued": "http://purl.org/dc/terms/issued",
        "medium": {
            "@id": "http://purl.org/dc/terms/medium",
            "@type": "@id"
        },
        "umbel": "http://umbel.org/umbel#",
        "rdau": "http://www.rdaregistry.info/Elements/u/#",
        "owl": "http://www.w3.org/2002/07/owl#",
        "dcterms": "http://purl.org/dc/terms/",
        "bibo": "http://purl.org/ontology/bibo/",
        "rdam": "http://www.rdaregistry.info/Elements/m/#",
        "gnd": "http://d-nb.info/gnd/",
        "isbd": "http://iflastandards.info/ns/isbd/elements/",
        "rda": "http://rdvocab.info/",
        "doi": "https://doi.org/"
    }
}