{
    "@graph": [
        {
            "@id": "gnd:1260779599",
            "sameAs": "Balles, Lukas"
        },
        {
            "@id": "gnd:36187-2",
            "sameAs": "Eberhard Karls Universit\u00e4t T\u00fcbingen"
        },
        {
            "@id": "https://www.tib.eu/de/suchen/id/TIBKAT%3A1807555933",
            "@type": "bibo:Thesis",
            "P1053": "1 Online-Ressource (xii, 143 Seiten)",
            "description": "Illustrationen",
            "identifier": [
                "(ppn)1807555933",
                "(doi)10.15496/publikation-69382",
                "(firstid)KXP:1807555933"
            ],
            "subject": [
                "(classificationName=ddc)519.6",
                "(classificationName=linseach:mapping)mat",
                "(classificationName=ddc-dbn)510"
            ],
            "title": "Noise-aware stochastic optimization",
            "abstract": [
                "First-order stochastic optimization algorithms like stochastic gradient descent (SGD) are the workhorse of modern machine learning. With their simplicity and low per-iteration cost, they have powered the immense success of deep artificial neural network models. Surprisingly, these stochastic optimization methods are essentially unaware of stochasticity. Neither do they collect information about the stochastic noise associated with their gradient evaluations, nor do they have explicit mechanisms to adjust their behavior accordingly. This thesis presents approaches to make stochastic optimization methods noise-aware using estimates of the (co-)variance of stochastic gradients. First, we show how such variance estimates can be used to automatically adapt the minibatch size for SGD, i.e., the number of data points sampled in each iteration. This can replace the usual decreasing step size schedule required for convergence, which is much more challenging to automate. We highlight that both approaches can be viewed through the same lens of reducing the mean squared error of the gradient estimate. Next, we identify an implicit variance adaptation mechanism in the ubiquitous Adam method. In particular, we show that it can be seen as a version of sign-SGD with a coordinatewise \u201cdamping\u201d based on the stochastic gradient\u2019s signal-to-noise ratio. We make this variance adaptation mechanism explicit, formalize it, and transfer it from sign-SGD to SGD. Finally, we critically discuss a family of methods that preconditions stochastic gradient descent updates with the so-called \u201cempirical Fisher\u201d matrix, which is closely related to the stochastic gradient covariance matrix. This is usually motivated from information- geometric considerations as an approximation to the Fisher information matrix. We caution against this argument and show that the empirical Fisher approximation has fundamental theoretical flaws. We argue that preconditioning with the empirical Fisher is better understood as a form of variance adaptation.",
                "Sochastische Optimierungsverfahren erster Ordnung, wie zum Beispiel das stochastische Gradientenverfahren (stochastic gradient descent, SGD), sind das Arbeitstier des modernen maschinellen Lernens. Mit ihrer Einfachheit und niedrigen Kosten pro Iteration haben sie den immensen Erfolg k\u00fcnstlicher neuronaler Netze ma\u00dfgeblich vorangetrieben. \u00dcberraschender Weise sind diese stochastischen Optimierungsmethoden blind gegen\u00fcber Stochastizit\u00e4t. Weder sammeln sie Informationen \u00fcber das stochastische Rauschen der verwendeten Gradientenauswertungen, noch verf\u00fcgen sie \u00fcber explizite Mechanismen, um ihr Verhalten an dieses Rauschen anzupassen. Diese Arbeit pr\u00e4sentiert Ans\u00e4tze, stochastischen Optimierungsverfahren mittels Sch\u00e4tzung der (Ko- )Varianz der stochastischen Gradienten ein \u201cBewusstsein\u201d f\u00fcr dieses Rauschen zu geben. Zuerst zeigen wir, wie solche Varianzsch\u00e4tzungen genutzt werden k\u00f6nnen, um die sogenannte Minibatchgr\u00f6\u00dfe bei SGD automatisch anzupassen. Dies kann eine \u00fcblicherweise verwendete ab- nehmende Schrittweite ersetzten, welche ihrerseits sehr viel schwerer zu automatisieren ist. Wir stellen heraus, dass beide Herangehensweisen aus einer gemeinsamen Perspektive betrachtet werden k\u00f6nnen, n\u00e4mlich als Reduktion der mittleren quadratischen Abweichung des Gradientensch\u00e4tzers. Als n\u00e4chstes identifizieren wir in der au\u00dfergew\u00f6hnlich popul\u00e4ren Adam-Methode einen impliziten Varianzadaptierungsmechanismus. Wir betrachten Adam als eine Version von sign-SGD mit koordinatenweiser \u201cD\u00e4mpfung\u201d auf Basis des Signal-zu-Rausch-Verh\u00e4ltnisses des stochastischen Gradienten. Wir machen diesen Mechanismus explizit, formalisieren ihn, und \u00fcbertragen ihn von sign-SGD zu SGD. Abschlie\u00dfend folgt eine kritische Diskussion einer Methodenfamilie, welche SGD mit der sogenannten \u201cempirischen Fisher-Matrix\u201d pr\u00e4konditionert. Diese Matrix ist eng mit der Kovarianzmatrix des stochastischen Gradienten verwandt. Die empirische Fisher-Matrix wird \u00fcblicherweise als Approximation f\u00fcr die Fisher-Matrix und somit aus informationsgeometrischen \u00dcberlegungen motiviert. Wir kritisieren dieses Argument und zeigen, dass diese Approximation fundamentale theoretische Schw\u00e4chen hat. Wir argumentieren, dass die Pr\u00e4konditionierung mit der empirischen Fisher-Matrix besser als eine Form von Varianzadaptierung gesehen werden sollte."
            ],
            "contributor": "Technische Informationsbibliothek (TIB)",
            "creator": [
                "gnd:36187-2",
                "gnd:1260779599"
            ],
            "isPartOf": "(collectioncode)GBV-ODiss",
            "issued": "2021",
            "language": "http://id.loc.gov/vocabulary/iso639-1/en",
            "license": "open access",
            "medium": "rda:termList/RDACarrierType/1018",
            "isLike": "doi:10.15496/publikation-69382",
            "P60163": "T\u00fcbingen"
        }
    ],
    "@id": "urn:x-arq:DefaultGraphNode",
    "@context": {
        "abstract": "http://purl.org/dc/terms/abstract",
        "isPartOf": "http://purl.org/dc/terms/isPartOf",
        "creator": {
            "@id": "http://purl.org/dc/terms/creator",
            "@type": "@id"
        },
        "issued": "http://purl.org/dc/terms/issued",
        "description": "http://purl.org/dc/elements/1.1/description",
        "isLike": {
            "@id": "http://umbel.org/umbel#isLike",
            "@type": "@id"
        },
        "P60163": "http://www.rdaregistry.info/Elements/u/#P60163",
        "identifier": "http://purl.org/dc/elements/1.1/identifier",
        "title": "http://purl.org/dc/elements/1.1/title",
        "subject": "http://purl.org/dc/elements/1.1/subject",
        "language": {
            "@id": "http://purl.org/dc/terms/language",
            "@type": "@id"
        },
        "medium": {
            "@id": "http://purl.org/dc/terms/medium",
            "@type": "@id"
        },
        "license": "http://purl.org/dc/terms/license",
        "P1053": "http://iflastandards.info/ns/isbd/elements/P1053",
        "contributor": "http://purl.org/dc/terms/contributor",
        "sameAs": "http://www.w3.org/2002/07/owl#sameAs",
        "umbel": "http://umbel.org/umbel#",
        "rdau": "http://www.rdaregistry.info/Elements/u/#",
        "owl": "http://www.w3.org/2002/07/owl#",
        "dcterms": "http://purl.org/dc/terms/",
        "bibo": "http://purl.org/ontology/bibo/",
        "rdam": "http://www.rdaregistry.info/Elements/m/#",
        "gnd": "http://d-nb.info/gnd/",
        "isbd": "http://iflastandards.info/ns/isbd/elements/",
        "rda": "http://rdvocab.info/",
        "doi": "https://doi.org/"
    }
}