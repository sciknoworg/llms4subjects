{
    "@graph": [
        {
            "@id": "gnd:1269140086",
            "sameAs": "Lohaus, Michael"
        },
        {
            "@id": "gnd:36187-2",
            "sameAs": "Eberhard Karls Universit\u00e4t T\u00fcbingen"
        },
        {
            "@id": "https://www.tib.eu/de/suchen/id/TIBKAT%3A1817805819",
            "@type": "bibo:Thesis",
            "P1053": "1 Online-Ressource (ix, 123 Seiten)",
            "description": "Illustrationen",
            "identifier": [
                "(doi)10.15496/publikation-73431",
                "(ppn)1817805819",
                "(firstid)KXP:1817805819"
            ],
            "subject": [
                "Maschinelles Lernen",
                "(classificationName=ddc-dbn)340",
                "Fairness",
                "(classificationName=linseach:mapping)jur"
            ],
            "title": "Limitations of fairness in machine learning",
            "abstract": [
                "Die Frage des sozial verantwortlichen maschinellen Lernens ist so dringlich wie nie zuvor. Ein ganzer Bereich des maschinellen Lernens hat es sich zur Aufgabe gemacht, die gesellschaftlichen Aspekte automatisierter Entscheidungssysteme zu untersuchen und technische L\u00f6sungen f\u00fcr algorithmische Fairness bereitzustellen. Jeder Versuch, die Fairness von Algorithmen zu verbessern, muss jedoch unter dem Blickwinkel eines m\u00f6glichen gesellschaftlichen Schadens untersucht werden. In dieser Arbeit untersuchen wir bestehende Ans\u00e4tze f\u00fcr faire Klassifikationsverfahren und beleuchten deren unterschiedliche Einschr\u00e4nkungen. Als Erstes zeigen wir, dass Relaxierungen von Fairness, die verwendet werden, um den Lernprozess von fairen Modellen zu vereinfachen, zu grob sind, da der endg\u00fcltige Klassifikator unfair sein kann, obwohl die relaxierte Bedingung erf\u00fcllt ist. Als Antwort darauf schlagen wir eine neue und beweisbar faire Methode vor, die die Fairness-Relaxierungen in einer stark konvexen Formulierung wiederverwendet. Zweitens beobachten wir ein erh\u00f6htes Bewusstsein f\u00fcr gesch\u00fctzte Merkmale wie Rasse oder Geschlecht in der letzten Schicht tiefer neuronaler Netze, wenn wir sie f\u00fcr faire Ergebnisse regularisieren. Auf Basis dieser Beobachtung konstruieren wir ein neuronales Netz, das die Eingabepunkte wegen gesch\u00fctzter pers\u00f6nlicher Merkmale explizit unterschiedlich behandelt. Mit dieser expliziten Formulierung k\u00f6nnen wir die Vorhersagen eines fairen neuronalen Netzwerks replizieren. Wir behaupten, dass sowohl das faire neuronale Netzwerk als auch die explizite Formulierung Disparate Treatment aufzeigen---eine Form der Diskriminierung in vielen Antidiskriminierungsgesetzen. Drittens betrachten wir die Fairness-Eigenschaften des Mehrheitsvotums - einer beliebten Ensemble-Methode zur Aggregation mehrerer Modelle maschinellen Lernens. Wir untersuchen algorithmisch Worst-Case-Garantien f\u00fcr die Fairness des Mehrheitsvotums, wenn es aus mehreren Klassifikatoren besteht, die selbst schon fair sind. Unter starken Unabh\u00e4ngigkeitsannahmen an die Klassifikatoren k\u00f6nnen wir ein faires Mehrheitsvotum garantieren. Ohne irgendwelche Annahmen an die Klassifikatoren kann ein faires Mehrheitsvotum im Allgemeinen nicht garantiert werden, aber es sind verschiedene Fairness-Regime m\u00f6glich: Einerseits kann die Verwendung fairer Klassifikatoren die Fairness-Garantien f\u00fcr den Worst-Case verbessern. Andererseits kann es sein, dass das Mehrheitsvotum \u00fcberhaupt nicht fair ist.",
                "The issue of socially responsible machine learning has never been more pressing. An entire field of machine learning is dedicated to investigating the societal aspects of automated decision-making systems and providing technical solutions for algorithmic fairness. However, any attempt to improve the fairness of algorithms must be examined under the lens of potential societal harm. In this thesis, we study existing approaches to fair classification and shed light on their various limitations. First, we show that relaxations of fairness constraints used to simplify the learning process of fair models are too coarse, since the final classifier may be distinctly unfair even though the relaxed constraint is satisfied. In response, we propose a new and provably fair method that incorporates the fairness relaxations in a strongly convex formulation. Second, we observe an increased awareness of protected attributes such as race or gender in the last layer of deep neural networks when we regularize them for fair outcomes. Based on this observation, we construct a neural network that explicitly treats input points differently because of protected personal characteristics. With this explicit formulation, we can replicate the predictions of a fair neural network. We argue that both the fair neural network and the explicit formulation demonstrate disparate treatment-a form of discrimination in anti-discrimination laws. Third, we consider fairness properties of the majority vote-a popular ensemble method for aggregating multiple machine learning models to obtain more accurate and robust decisions. We algorithmically investigate worst-case fairness guarantees of the majority vote when it consists of multiple classifiers that are themselves already fair. Under strong independence assumptions on the classifiers, we can guarantee a fair majority vote. Without any assumptions on the classifiers, a fair majority vote cannot be guaranteed in general, but different fairness regimes are possible: on the one hand, using fair classifiers may improve the worst-case fairness guarantees. On the other hand, the majority vote may not be fair at all."
            ],
            "contributor": "Technische Informationsbibliothek (TIB)",
            "creator": [
                "gnd:1269140086",
                "gnd:36187-2"
            ],
            "isPartOf": "(collectioncode)GBV-ODiss",
            "issued": "2022",
            "language": "http://id.loc.gov/vocabulary/iso639-1/en",
            "license": "open access",
            "medium": "rda:termList/RDACarrierType/1018",
            "isLike": "doi:10.15496/publikation-73431",
            "P60163": "T\u00fcbingen"
        }
    ],
    "@id": "urn:x-arq:DefaultGraphNode",
    "@context": {
        "creator": {
            "@id": "http://purl.org/dc/terms/creator",
            "@type": "@id"
        },
        "abstract": "http://purl.org/dc/terms/abstract",
        "license": "http://purl.org/dc/terms/license",
        "identifier": "http://purl.org/dc/elements/1.1/identifier",
        "description": "http://purl.org/dc/elements/1.1/description",
        "subject": "http://purl.org/dc/elements/1.1/subject",
        "language": {
            "@id": "http://purl.org/dc/terms/language",
            "@type": "@id"
        },
        "isPartOf": "http://purl.org/dc/terms/isPartOf",
        "isLike": {
            "@id": "http://umbel.org/umbel#isLike",
            "@type": "@id"
        },
        "contributor": "http://purl.org/dc/terms/contributor",
        "title": "http://purl.org/dc/elements/1.1/title",
        "medium": {
            "@id": "http://purl.org/dc/terms/medium",
            "@type": "@id"
        },
        "P1053": "http://iflastandards.info/ns/isbd/elements/P1053",
        "issued": "http://purl.org/dc/terms/issued",
        "P60163": "http://www.rdaregistry.info/Elements/u/#P60163",
        "sameAs": "http://www.w3.org/2002/07/owl#sameAs",
        "umbel": "http://umbel.org/umbel#",
        "rdau": "http://www.rdaregistry.info/Elements/u/#",
        "owl": "http://www.w3.org/2002/07/owl#",
        "dcterms": "http://purl.org/dc/terms/",
        "bibo": "http://purl.org/ontology/bibo/",
        "rdam": "http://www.rdaregistry.info/Elements/m/#",
        "gnd": "http://d-nb.info/gnd/",
        "isbd": "http://iflastandards.info/ns/isbd/elements/",
        "rda": "http://rdvocab.info/",
        "doi": "https://doi.org/"
    }
}