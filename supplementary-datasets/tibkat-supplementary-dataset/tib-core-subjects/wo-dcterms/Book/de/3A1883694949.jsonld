{
    "@graph": [
        {
            "@id": "gnd:1153501937",
            "sameAs": "Ozdemir, Sinan"
        },
        {
            "@id": "gnd:123178002",
            "sameAs": "Langenau, Frank"
        },
        {
            "@id": "https://www.tib.eu/de/suchen/id/TIBKAT%3A1883694949",
            "@type": "bibo:Book",
            "P1053": "1 Online-Ressource (271 Seiten)",
            "description": [
                "Illustrationen, Diagramme",
                "Campusweiter Zugriff (Universit\u00e4t Hannover) - Vervielf\u00e4ltigungen (z.B. Kopien, Downloads) sind nur von einzelnen Kapiteln oder Seiten und nur zum eigenen wissenschaftlichen Gebrauch erlaubt. Keine Weitergabe an Dritte. Kein systematisches Downloaden durch Robots"
            ],
            "identifier": [
                "(firstid)KEP:101400624",
                "(ppn)1883694949",
                "(isbn13)9783960108535",
                "(isbn13)9783960108542"
            ],
            "publisher": "O'Reilly",
            "subject": [
                "Natural Language Processing",
                "GPT-4",
                "OpenAI",
                "KI",
                "LangChain",
                "Transfer Learning",
                "AI",
                "Q&A",
                "BERT",
                "NLP",
                "Cohere",
                "Transformer",
                "BART",
                "EleutherAI",
                "Attention",
                "Question Answering",
                "T5",
                "Prompt Engineering",
                "Machine Learning",
                "LLaMa",
                "(classificationName=linseach:lexicon)inf",
                "Bard",
                "Python",
                "Embeddings"
            ],
            "title": "Praxiseinstieg Large Language Models : Strategien und Best Practices f\u00fcr den Einsatz von ChatGPT und anderen LLms",
            "abstract": "Large Language Models (LLMs) wie ChatGPT sind enorm leistungsf\u00e4hig, aber auch sehr komplex. Praktikerinnen und Praktiker stehen daher vor vielf\u00e4ltigen Herausforderungen, wenn sie LLMs in ihre eigenen Anwendungen integrieren wollen. In dieser Einf\u00fchrung r\u00e4umt Data Scientist und KI-Unternehmer Sinan Ozdemir diese H\u00fcrden aus dem Weg und bietet einen Leitfaden f\u00fcr den Einsatz von LLMs zur L\u00f6sung praktischer Probleme des Natural Language Processings.Sinan Ozdemir hat alles zusammengestellt, was Sie f\u00fcr den Einstieg ben\u00f6tigen: Schritt-f\u00fcr-Schritt-Anleitungen, Best Practices, Fallstudien aus der Praxis, \u00dcbungen und vieles mehr. Er stellt die Funktionsweise von LLMs vor und unterst\u00fctzt Sie so dabei, das f\u00fcr Ihre Anwendung passende Modell und geeignete Datenformate und Parameter auszuw\u00e4hlen. Dabei zeigt er das Potenzial sowohl von Closed-Source- als auch von Open-Source-LLMs wie GPT-3, GPT-4 und ChatGPT, BERT und T5, GPT-J und GPT-Neo, Cohere sowie BART.\u2022 Lernen Sie die Schl\u00fcsselkonzepte kennen: Transfer Learning, Feintuning, Attention, Embeddings, Tokenisierung und mehr\u2022 Nutzen Sie APIs und Python, um LLMs an Ihre Anforderungen anzupassen\u2022 Beherrschen Sie Prompt-Engineering-Techniken wie Ausgabe-Strukturierung, Gedankenketten und Few-Shot-Prompting\u2022 Passen Sie LLM-Embeddings an, um eine Empfehlungsengine mit eigenen Benutzerdaten neu zu erstellen\u2022 Konstruieren Sie multimodale Transformer-Architekturen mithilfe von Open-Source-LLMs\u2022 Optimieren Sie LLMs mit Reinforcement Learning from Human and AI Feedback (RLHF/RLAIF)\u2022 Deployen Sie Prompts und benutzerdefinierte, feingetunte LLMs in die Cloud",
            "alternative": "Quick start guide to large language models. Strategies and best practices for using ChatGPT and other LLMs",
            "contributor": [
                "Technische Informationsbibliothek (TIB)",
                {
                    "@id": "gnd:123178002"
                }
            ],
            "creator": "gnd:1153501937",
            "isPartOf": "(collectioncode)ZDB-219-ODS",
            "issued": "2024",
            "language": "http://id.loc.gov/vocabulary/iso639-1/de",
            "license": "commercial licence",
            "medium": "rda:termList/RDACarrierType/1018",
            "P30128": "Animals",
            "P60163": "Heidelberg"
        }
    ],
    "@id": "urn:x-arq:DefaultGraphNode",
    "@context": {
        "sameAs": "http://www.w3.org/2002/07/owl#sameAs",
        "publisher": "http://purl.org/dc/elements/1.1/publisher",
        "subject": "http://purl.org/dc/elements/1.1/subject",
        "description": "http://purl.org/dc/elements/1.1/description",
        "P30128": "http://www.rdaregistry.info/Elements/m/#P30128",
        "title": "http://purl.org/dc/elements/1.1/title",
        "contributor": "http://purl.org/dc/terms/contributor",
        "abstract": "http://purl.org/dc/terms/abstract",
        "alternative": "http://purl.org/dc/terms/alternative",
        "identifier": "http://purl.org/dc/elements/1.1/identifier",
        "language": {
            "@id": "http://purl.org/dc/terms/language",
            "@type": "@id"
        },
        "P60163": "http://www.rdaregistry.info/Elements/u/#P60163",
        "license": "http://purl.org/dc/terms/license",
        "P1053": "http://iflastandards.info/ns/isbd/elements/P1053",
        "isPartOf": "http://purl.org/dc/terms/isPartOf",
        "issued": "http://purl.org/dc/terms/issued",
        "medium": {
            "@id": "http://purl.org/dc/terms/medium",
            "@type": "@id"
        },
        "creator": {
            "@id": "http://purl.org/dc/terms/creator",
            "@type": "@id"
        },
        "umbel": "http://umbel.org/umbel#",
        "rdau": "http://www.rdaregistry.info/Elements/u/#",
        "owl": "http://www.w3.org/2002/07/owl#",
        "dcterms": "http://purl.org/dc/terms/",
        "bibo": "http://purl.org/ontology/bibo/",
        "rdam": "http://www.rdaregistry.info/Elements/m/#",
        "gnd": "http://d-nb.info/gnd/",
        "isbd": "http://iflastandards.info/ns/isbd/elements/",
        "rda": "http://rdvocab.info/",
        "doi": "https://doi.org/"
    }
}