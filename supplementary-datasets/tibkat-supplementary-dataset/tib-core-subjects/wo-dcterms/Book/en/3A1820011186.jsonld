{
    "@graph": [
        {
            "@id": "https://www.tib.eu/de/suchen/id/TIBKAT%3A1820011186",
            "@type": "bibo:Book",
            "P1053": "1 Online-Ressource(XI, 165 p. 80 illus.)",
            "creator": "Jain, Shashank Mohan",
            "description": [
                "Campusweiter Zugriff (Universit\u00e4t Hannover) - Vervielf\u00e4ltigungen (z.B. Kopien, Downloads) sind nur von einzelnen Kapiteln oder Seiten und nur zum eigenen wissenschaftlichen Gebrauch erlaubt. Keine Weitergabe an Dritte. Kein systematisches Downloaden durch Robots.",
                "Erworben aus Studienqualit\u00e4tsmitteln"
            ],
            "identifier": [
                "(doi)10.1007/978-1-4842-8844-3",
                "(firstid)KEP:082334897",
                "(isbn13)9781484288443",
                "(ppn)1820011186"
            ],
            "publisher": "Apress",
            "subject": [
                "Artificial intelligence\u2014Data processing.",
                "Machine learning.",
                "Artificial intelligence.",
                "Python (Computer program language).",
                "(classificationName=linseach:mapping)inf",
                "(classificationName=ddc)006.3"
            ],
            "title": "Introduction to Transformers for NLP : With the Hugging Face Library and Models to Solve Problems",
            "abstract": [
                "Chapter 1: Introduction to Language Models -- Chapter 2: Introduction to Transformers -- Chapter 3: BERT -- Chapter 4: Hugging Face -- Chapter 5: Tasks Using the Huggingface Library -- Chapter 6: Fine-Tuning Pre-Trained Models -- Appendix A: Vision Transformers.",
                "Get a hands-on introduction to Transformer architecture using the Hugging Face library. This book explains how Transformers are changing the AI domain, particularly in the area of natural language processing. This book covers Transformer architecture and its relevance in natural language processing (NLP). It starts with an introduction to NLP and a progression of language models from n-grams to a Transformer-based architecture. Next, it offers some basic Transformers examples using the Google colab engine. Then, it introduces the Hugging Face ecosystem and the different libraries and models provided by it. Moving forward, it explains language models such as Google BERT with some examples before providing a deep dive into Hugging Face API using different language models to address tasks such as sentence classification, sentiment analysis, summarization, and text generation. After completing Introduction to Transformers for NLP, you will understand Transformer concepts and be able to solve problems using the Hugging Face library. You will: Understand language models and their importance in NLP and NLU (Natural Language Understanding) Master Transformer architecture through practical examples Use the Hugging Face library in Transformer-based language models Create a simple code generator in Python based on Transformer architecture."
            ],
            "contributor": "Technische Informationsbibliothek (TIB)",
            "isPartOf": [
                "(collectioncode)ZDB-2-CWD",
                "(collectioncode)ZDB-2-SEB",
                "(collectioncode)ZDB-2-SXPC"
            ],
            "issued": "2022",
            "language": "http://id.loc.gov/vocabulary/iso639-1/en",
            "license": "commercial licence",
            "medium": "rda:termList/RDACarrierType/1018",
            "isLike": "doi:10.1007/978-1-4842-8844-3",
            "P60163": "Berkeley, CA"
        }
    ],
    "@id": "urn:x-arq:DefaultGraphNode",
    "@context": {
        "subject": "http://purl.org/dc/elements/1.1/subject",
        "license": "http://purl.org/dc/terms/license",
        "P60163": "http://www.rdaregistry.info/Elements/u/#P60163",
        "description": "http://purl.org/dc/elements/1.1/description",
        "creator": "http://purl.org/dc/elements/1.1/creator",
        "language": {
            "@id": "http://purl.org/dc/terms/language",
            "@type": "@id"
        },
        "title": "http://purl.org/dc/elements/1.1/title",
        "isLike": {
            "@id": "http://umbel.org/umbel#isLike",
            "@type": "@id"
        },
        "abstract": "http://purl.org/dc/terms/abstract",
        "identifier": "http://purl.org/dc/elements/1.1/identifier",
        "contributor": "http://purl.org/dc/terms/contributor",
        "isPartOf": "http://purl.org/dc/terms/isPartOf",
        "medium": {
            "@id": "http://purl.org/dc/terms/medium",
            "@type": "@id"
        },
        "publisher": "http://purl.org/dc/elements/1.1/publisher",
        "issued": "http://purl.org/dc/terms/issued",
        "P1053": "http://iflastandards.info/ns/isbd/elements/P1053",
        "umbel": "http://umbel.org/umbel#",
        "rdau": "http://www.rdaregistry.info/Elements/u/#",
        "owl": "http://www.w3.org/2002/07/owl#",
        "dcterms": "http://purl.org/dc/terms/",
        "bibo": "http://purl.org/ontology/bibo/",
        "rdam": "http://www.rdaregistry.info/Elements/m/#",
        "gnd": "http://d-nb.info/gnd/",
        "isbd": "http://iflastandards.info/ns/isbd/elements/",
        "rda": "http://rdvocab.info/",
        "doi": "https://doi.org/"
    }
}