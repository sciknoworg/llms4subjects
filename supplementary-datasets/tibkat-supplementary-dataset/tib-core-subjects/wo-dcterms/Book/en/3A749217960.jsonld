{
    "@graph": [
        {
            "@id": "https://www.tib.eu/de/suchen/id/TIBKAT%3A749217960",
            "@type": "bibo:Book",
            "P1053": "Online-Ressource (172 p)",
            "creator": "Sutton, Richard S.",
            "description": [
                "Campusweiter Zugriff (Universit\u00e4t Hannover). - Vervielf\u00e4ltigungen (z.B. Kopien, Downloads) sind nur von einzelnen Kapiteln oder Seiten und nur zum eigenen wissenschaftlichen Gebrauch erlaubt. Keine Weitergabe an Dritte. Kein systematisches Downloaden durch Robots.",
                "digital"
            ],
            "identifier": [
                "(isbn13)9781461536185",
                "(ppn)749217960",
                "(firstid)GBV:749217960",
                "(doi)10.1007/978-1-4615-3618-5"
            ],
            "publisher": "Springer",
            "subject": [
                "System theory.",
                "Computer science",
                "Mathematical physics.",
                "(classificationName=ddc)006.3",
                "Artificial intelligence",
                "(classificationName=linseach:mapping)mas",
                "(classificationName=loc)TJ210.2-211.495",
                "(classificationName=loc)Q334-342"
            ],
            "title": "Reinforcement Learning",
            "abstract": "Reinforcement learning is the learning of a mapping from situations to actions so as to maximize a scalar reward or reinforcement signal. The learner is not told which action to take, as in most forms of machine learning, but instead must discover which actions yield the highest reward by trying them. In the most interesting and challenging cases, actions may affect not only the immediate reward, but also the next situation, and through that all subsequent rewards. These two characteristics -- trial-and-error search and delayed reward -- are the most important distinguishing features of reinforcement learning. Reinforcement learning is both a new and a very old topic in AI. The term appears to have been coined by Minsk (1961), and independently in control theory by Walz and Fu (1965). The earliest machine learning research now viewed as directly relevant was Samuel's (1959) checker player, which used temporal-difference learning to manage delayed reward much as it is used today. Of course learning and reinforcement have been studied in psychology for almost a century, and that work has had a very strong impact on the AI/engineering work. One could in fact consider all of reinforcement learning to be simply the reverse engineering of certain psychological learning processes (e.g. operant conditioning and secondary reinforcement). Reinforcement Learning is an edited volume of original research, comprising seven invited contributions by leading researchers",
            "contributor": "Technische Informationsbibliothek (TIB)",
            "isPartOf": [
                "(collectioncode)ZDB-2-SXE",
                "(collectioncode)ZDB-2-BAE",
                "(collectioncode)ZDB-2-SEB",
                "(collectioncode)ZDB-2-ENG"
            ],
            "issued": "1992",
            "language": "http://id.loc.gov/vocabulary/iso639-1/en",
            "license": "commercial licence",
            "medium": "rda:termList/RDACarrierType/1018",
            "volume": "173",
            "isLike": "doi:10.1007/978-1-4615-3618-5",
            "P30128": "The Springer International Series in Engineering and Computer Science, Knowledge Representation, Learning and Expert Systems",
            "P60163": "Boston, MA"
        }
    ],
    "@id": "urn:x-arq:DefaultGraphNode",
    "@context": {
        "subject": "http://purl.org/dc/elements/1.1/subject",
        "license": "http://purl.org/dc/terms/license",
        "P30128": "http://www.rdaregistry.info/Elements/m/#P30128",
        "creator": "http://purl.org/dc/elements/1.1/creator",
        "publisher": "http://purl.org/dc/elements/1.1/publisher",
        "title": "http://purl.org/dc/elements/1.1/title",
        "isPartOf": "http://purl.org/dc/terms/isPartOf",
        "description": "http://purl.org/dc/elements/1.1/description",
        "volume": "http://purl.org/ontology/bibo/volume",
        "P60163": "http://www.rdaregistry.info/Elements/u/#P60163",
        "identifier": "http://purl.org/dc/elements/1.1/identifier",
        "contributor": "http://purl.org/dc/terms/contributor",
        "isLike": {
            "@id": "http://umbel.org/umbel#isLike",
            "@type": "@id"
        },
        "issued": "http://purl.org/dc/terms/issued",
        "abstract": "http://purl.org/dc/terms/abstract",
        "P1053": "http://iflastandards.info/ns/isbd/elements/P1053",
        "language": {
            "@id": "http://purl.org/dc/terms/language",
            "@type": "@id"
        },
        "medium": {
            "@id": "http://purl.org/dc/terms/medium",
            "@type": "@id"
        },
        "umbel": "http://umbel.org/umbel#",
        "rdau": "http://www.rdaregistry.info/Elements/u/#",
        "owl": "http://www.w3.org/2002/07/owl#",
        "dcterms": "http://purl.org/dc/terms/",
        "bibo": "http://purl.org/ontology/bibo/",
        "rdam": "http://www.rdaregistry.info/Elements/m/#",
        "gnd": "http://d-nb.info/gnd/",
        "isbd": "http://iflastandards.info/ns/isbd/elements/",
        "rda": "http://rdvocab.info/",
        "doi": "https://doi.org/"
    }
}