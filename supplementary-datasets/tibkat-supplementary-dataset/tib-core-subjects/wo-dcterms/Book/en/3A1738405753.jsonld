{
    "@graph": [
        {
            "@id": "https://www.tib.eu/de/suchen/id/TIBKAT%3A1738405753",
            "@type": "bibo:Book",
            "P1053": "1 Online-Ressource",
            "contributor": [
                "Mullainathan, Sendhil",
                "Kleinberg, Jon",
                "Ludwig, Jens"
            ],
            "creator": [
                "Rambachan, Ashesh",
                "National Bureau of Economic Research"
            ],
            "description": [
                "Campusweiter Zugriff (Universit\u00e4t Hannover) - Vervielf\u00e4ltigungen (z.B. Kopien, Downloads) sind nur von einzelnen Kapiteln oder Seiten und nur zum eigenen wissenschaftlichen Gebrauch erlaubt. Keine Weitergabe an Dritte. Kein systematisches Downloaden durch Robots.",
                "illustrations (black and white)"
            ],
            "identifier": [
                "(firstid)KEP:059521651",
                "(ppn)1738405753",
                "(doi)10.3386/w27111"
            ],
            "publisher": "National Bureau of Economic Research",
            "subject": "(classificationName=linseach:automatic)inf",
            "title": "An Economic Approach to Regulating Algorithms",
            "abstract": "There is growing concern about \"algorithmic bias\" - that predictive algorithms used in decision-making might bake in or exacerbate discrimination in society. When will these \"biases\" arise? What should be done about them? We argue that such questions are naturally answered using the tools of welfare economics: a social welfare function for the policymaker, a private objective function for the algorithm designer and a model of their information sets and interaction. We build such a model that allows the training data to exhibit a wide range of \"biases.\" Prevailing wisdom is that biased data change how the algorithm is trained and whether an algorithm should be used at all. In contrast, we find two striking irrelevance results. First, when the social planner builds the algorithm, her equity preference has no effect on the training procedure. So long as the data, however biased, contain signal, they will be used and the algorithm built on top will be the same. Any characteristic that is predictive of the outcome of interest, including group membership, will be used. Second, we study how the social planner regulates private (possibly discriminatory) actors building algorithms. Optimal regulation depends crucially on the disclosure regime. Absent disclosure, algorithms are regulated much like human decision-makers: disparate impact and disparate treatment rules dictate what is allowed. In contrast, under stringent disclosure of all underlying algorithmic inputs (data, training procedure and decision rule), once again we find an irrelevance result: private actors can use any predictive characteristic. Additionally, now algorithms strictly reduce the extent of discrimination against protected groups relative to a world in which humans make all the decisions. As these results run counter to prevailing wisdom on algorithmic bias, at a minimum, they provide a baseline set of assumptions that must be altered to generate different conclusions",
            "dcterms:contributor": "Technische Informationsbibliothek (TIB)",
            "isPartOf": "(collectioncode)ZDB-194-NBW",
            "issued": "2020",
            "language": "http://id.loc.gov/vocabulary/iso639-1/en",
            "license": "open access",
            "medium": "rda:termList/RDACarrierType/1018",
            "volume": "no. w27111",
            "isLike": "doi:10.3386/w27111",
            "P30128": "NBER working paper series",
            "P60163": "Cambridge, Mass"
        }
    ],
    "@id": "urn:x-arq:DefaultGraphNode",
    "@context": {
        "identifier": "http://purl.org/dc/elements/1.1/identifier",
        "abstract": "http://purl.org/dc/terms/abstract",
        "issued": "http://purl.org/dc/terms/issued",
        "contributor": "http://purl.org/dc/elements/1.1/contributor",
        "creator": "http://purl.org/dc/elements/1.1/creator",
        "isPartOf": "http://purl.org/dc/terms/isPartOf",
        "volume": "http://purl.org/ontology/bibo/volume",
        "description": "http://purl.org/dc/elements/1.1/description",
        "language": {
            "@id": "http://purl.org/dc/terms/language",
            "@type": "@id"
        },
        "title": "http://purl.org/dc/elements/1.1/title",
        "P30128": "http://www.rdaregistry.info/Elements/m/#P30128",
        "P60163": "http://www.rdaregistry.info/Elements/u/#P60163",
        "P1053": "http://iflastandards.info/ns/isbd/elements/P1053",
        "license": "http://purl.org/dc/terms/license",
        "medium": {
            "@id": "http://purl.org/dc/terms/medium",
            "@type": "@id"
        },
        "isLike": {
            "@id": "http://umbel.org/umbel#isLike",
            "@type": "@id"
        },
        "subject": "http://purl.org/dc/elements/1.1/subject",
        "publisher": "http://purl.org/dc/elements/1.1/publisher",
        "umbel": "http://umbel.org/umbel#",
        "rdau": "http://www.rdaregistry.info/Elements/u/#",
        "owl": "http://www.w3.org/2002/07/owl#",
        "dcterms": "http://purl.org/dc/terms/",
        "bibo": "http://purl.org/ontology/bibo/",
        "rdam": "http://www.rdaregistry.info/Elements/m/#",
        "gnd": "http://d-nb.info/gnd/",
        "isbd": "http://iflastandards.info/ns/isbd/elements/",
        "rda": "http://rdvocab.info/",
        "doi": "https://doi.org/"
    }
}