{
    "@graph": [
        {
            "@id": "gnd:1262019222",
            "sameAs": "Bechtle, Sarah Maria Elisabeth"
        },
        {
            "@id": "gnd:36187-2",
            "sameAs": "Eberhard Karls Universit\u00e4t T\u00fcbingen"
        },
        {
            "@id": "https://www.tib.eu/de/suchen/id/TIBKAT%3A1809424976",
            "@type": "bibo:Thesis",
            "P1053": "1 Online-Ressource (xv, 124 Seiten)",
            "description": "Illustrationen",
            "identifier": [
                "(ppn)1809424976",
                "(doi)10.15496/publikation-70393",
                "(firstid)KXP:1809424976"
            ],
            "subject": [
                "(classificationName=linseach:mapping)sow",
                "(classificationName=linseach:mapping)pae",
                "(classificationName=ddc-dbn)370"
            ],
            "title": "Lifelong learning in the real world",
            "abstract": "Damit Roboter uns Menschen in unserem ta \u0308glichen Leben unterstu \u0308tzen ko \u0308nnen, mu \u0308ssen sie von unserer Welt lernen und sich ihr anpassen. Die Anwendung des ma- schinellen Lernens auf reale Probleme stellt immer noch eine gro\u00dfe Herausforderung dar. Die reale Welt ist voller Ungewissheiten und vera \u0308ndert sich sta \u0308ndig, wodurch sich die Verteilung der Beobachtungen sta \u0308ndig verschiebt. Dies bedeutet, dass ein statischer Datensatz die reale Welt nicht lange darstellt. Au\u00dferdem ist die Daten- erfassung schwieriger und meistens kann immer nur eine kleine Menge an Daten fu \u0308r eine Aufgabe gesammelt werden. Aus diesem Grund lassen sich Algorithmen, die fu \u0308r einen statischen Problembereich und mithilfe gro\u00dfe Datensa \u0308tze entwickelt wurden, nicht gut auf diese Probleme u \u0308bertragen, auch wenn sie an den ju \u0308ngsten Erfolgsgeschichten des maschinellen Lernens beteiligt waren. Diese Arbeit befasst sich mit einer speziellen Art von Anwendung des maschinel- len Lernens in der realen Welt und zwar Roboter mit der Fa \u0308higkeit auszustatten, in der realen Welt zu lernen. Dazu muss der Roboter durch Interaktionen mit der Umgebung die zum Lernen notwendigen Daten sammeln. Diese Arbeit befasst sich mit der Frage, wie Repra \u0308sentationen erlernt werden ko \u0308nnen, die eine schnelle Gene- ralisierung und Anpassung an neue Aufgaben ermo \u0308glichen. Wie kann der Roboter auf bereits Erlerntes aufbauen, um wa \u0308hrend seiner gesamten Lebenszeit weiter aus Erfahrungen zu lernen? Dies wu \u0308rde dem Roboter eine Fa \u0308higkeit verleihen, die auch wir Menschen haben: schnell zuvor erlernte Fa \u0308higkeiten an neue Aufgaben anzu- passen. Ich mo \u0308chte verstehen, wie lebenslanges Lernen in diesem Rahmen mo \u0308glich ist: Der Roboter muss entscheiden, was er wann und wie lernen will.Insbesondere bescha \u0308ftigt sich die Arbeit mit der Frage, wie man Repra \u0308sentationen lernt, die zu schneller Generalisierung fu \u0308hren und dadurch ein schnelles Erlernen neuer Auf- gaben ermo \u0308glichen. Ich stu \u0308tze meine Forschung auf Erkenntnisse aus den Neuro- und Kognitionswissenschaften sowie der Entwicklungspsychologie und basiere mei- ne Ansa \u0308tze auf diesen Erkenntnissen, um Lernalgorithmen fu \u0308r Roboter zu entwi- ckeln. Der Inhalt dieser Arbeit na \u0308hert sich dieser Frage aus zwei verschiedenen, aber miteinander verbundenen Richtungen: 1. Modellbasiertes Lernen in der realen Welt: Eine Repra \u0308sentation der Um- gebung wird iterativ aus den durch den Roboter gesammelten Daten erlernt. Das menschliche Gehirn kann nicht jede Aufgabe von Grund auf neu zu er- lernen, deshalb baut es Kognitive Modelle der Umgebung (Lake et al., 2017). ix Kurzfassung Modelle versprechen eine flexible Anpassung an neue Aufgaben, ohne dass je- des Mal alles neu erlernt werden muss. Allerdings ko \u0308nnen die erlernten Model- le verzerrt, falsch oder alt sein. In Kapitel 2 (Bechtle et al., 2020a) zeigen wir, wie die Einbeziehung der Unsicherheit von Vorhersagen der gelernter dynami- schen Modelle wa \u0308hrend der Optimierung von Regelungsstrategien die Erkun- dung der Umgebung erleichtert und dadurch relevante Daten erfasst werden ko \u0308nnen. Das Ziel ist nun, nicht nur eine bestimmte Aufgabe zu erfu \u0308llen, aber auch die Unsicherheit in den Modellen aufzulo \u0308sen. Dadurch verbessert sich nicht nur das Modell, welches dann fu \u0308r andere Aufgaben genutzt werden kann, sondern auch die Leistung der Regelungsstrategie. Kapitel 3 (Bechtle et al., 2020c) stellt eine Verlustfunktion fu \u0308r das Lernen von Regelungsstrategien vor, die auch die Qualita \u0308t des Modells beru \u0308cksichtigt. In diesem Fall ist nicht nur von Bedeutung wie gut die gegebene Aufgabe erfu \u0308llt wurde, sondern auch wie genau das Modell Vorhersagen getro\u21b5en hat. Wir analysieren, wie diese Ver- lustfunktion die Datenerfassung erleichtert, um ein besseres Modell zu lernen und infolgedessen das Aufgabenlernen verbessert. Kapitel 4 (Bechtle et al., 2020b) betrachtet das Problem voreingenommener Modelle von einer anderen Perspektive: Durch die Nutzung von analytischem Vorwissen und die Kombi- nation mit einem datengesteuerten Ansatz wird ein visuelles Dynamikmodell erlernt, das bei Manipulationsaufgaben gute Ergebnisse erzielt, auch fu \u0308r neue Aufgaben. 2. Lernen wie man lernt in der realen Welt. Der Roboter lernt, wie er eine Aufgabe und damit eine Darstellung des Lernproblems erlernen kann. Menschen haben die bemerkenswerte Fa \u0308higkeit, kontinuierlich zu lernen und sich an neue Aufgaben anzupassen. Wir sind in der Lage zu lernen, wie man lernt. Davon inspiriert schlagen wir in Kapitel 5 ein vollsta \u0308ndig di\u21b5erenzier- bares Lernsystem vor, das Robotern ermo \u0308glicht, zu lernen, wie man lernt. In der Lernphase wird eine Verlustfunktion fu \u0308r eine Aufgabe aus Erfahrungen erlernt. Spa \u0308ter, nachdem der Roboter gelernt hat, wie man lernt, kann diese Funktion direkt auf neue Aufgaben angewendet werden. In (Das et al., 2020a) zeigen wir, wie dieses Lernprinzip auch bei einer Objektmanipulationsaufgabe angewendet werden kann, bei der der Roboter von menschlichen Demonstra- tionen gelernt hat. Lernen zu lernen ist ein grundlegender Bestandteil der menschlichen Intelligenz. Roboter mit dieser Fa \u0308higkeit auszustatten ist eine grundlegende Forschungsfrage. Diese beiden Richtungen befassen sich im Wesentlichen mit dem Erlernen von Re- pra \u0308sentationen, die sich schnell fu \u0308r neue Aufgaben und Szenarien benutzen lassen. Diese Form von Generalisierung ist eine Schlu \u0308sselfa \u0308higkeit, die es uns Menschen ermo \u0308glicht, kontinuierlich zu lernen.",
            "alternative": "Lifelong Learning in the Real World",
            "contributor": "Technische Informationsbibliothek (TIB)",
            "creator": [
                "gnd:1262019222",
                "gnd:36187-2"
            ],
            "isPartOf": "(collectioncode)GBV-ODiss",
            "issued": "2021",
            "language": "http://id.loc.gov/vocabulary/iso639-1/en",
            "license": "open access",
            "medium": "rda:termList/RDACarrierType/1018",
            "isLike": "doi:10.15496/publikation-70393",
            "P60163": "T\u00fcbingen"
        }
    ],
    "@id": "urn:x-arq:DefaultGraphNode",
    "@context": {
        "identifier": "http://purl.org/dc/elements/1.1/identifier",
        "language": {
            "@id": "http://purl.org/dc/terms/language",
            "@type": "@id"
        },
        "license": "http://purl.org/dc/terms/license",
        "abstract": "http://purl.org/dc/terms/abstract",
        "medium": {
            "@id": "http://purl.org/dc/terms/medium",
            "@type": "@id"
        },
        "subject": "http://purl.org/dc/elements/1.1/subject",
        "contributor": "http://purl.org/dc/terms/contributor",
        "isPartOf": "http://purl.org/dc/terms/isPartOf",
        "creator": {
            "@id": "http://purl.org/dc/terms/creator",
            "@type": "@id"
        },
        "alternative": "http://purl.org/dc/terms/alternative",
        "isLike": {
            "@id": "http://umbel.org/umbel#isLike",
            "@type": "@id"
        },
        "issued": "http://purl.org/dc/terms/issued",
        "title": "http://purl.org/dc/elements/1.1/title",
        "description": "http://purl.org/dc/elements/1.1/description",
        "P1053": "http://iflastandards.info/ns/isbd/elements/P1053",
        "P60163": "http://www.rdaregistry.info/Elements/u/#P60163",
        "sameAs": "http://www.w3.org/2002/07/owl#sameAs",
        "umbel": "http://umbel.org/umbel#",
        "rdau": "http://www.rdaregistry.info/Elements/u/#",
        "owl": "http://www.w3.org/2002/07/owl#",
        "dcterms": "http://purl.org/dc/terms/",
        "bibo": "http://purl.org/ontology/bibo/",
        "rdam": "http://www.rdaregistry.info/Elements/m/#",
        "gnd": "http://d-nb.info/gnd/",
        "isbd": "http://iflastandards.info/ns/isbd/elements/",
        "rda": "http://rdvocab.info/",
        "doi": "https://doi.org/"
    }
}