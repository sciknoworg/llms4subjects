{
    "@graph": [
        {
            "@id": "gnd:1106391888",
            "sameAs": "W\u00fclfing, Jan"
        },
        {
            "@id": "gnd:1135597375",
            "sameAs": "Deep learning"
        },
        {
            "@id": "gnd:4193754-5",
            "sameAs": "Maschinelles Lernen"
        },
        {
            "@id": "https://www.tib.eu/de/suchen/id/TIBKAT%3A1691663441",
            "@type": "bibo:Thesis",
            "P1053": "1 Online-Ressource (6, xvii, 199 Seiten)",
            "description": "Diagramme",
            "identifier": [
                "(contract)FRUB-opus-154753",
                "(firstid)DNB:1204826188",
                "(ppn)1691663441",
                "(doi)10.6094/UNIFR/154753"
            ],
            "http://purl.org/dc/elements/1.1/subject": [
                "(classificationName=ddc-dbn)004",
                "(classificationName=ddc)006.31",
                "(classificationName=linseach:mapping)inf"
            ],
            "title": "Stable deep reinforcement learning",
            "abstract": [
                "Abstract: Reinforcement Learning ist keine neue Disziplin im Bereich des maschinellen Lernens, hat jedoch gerade in den letzten Jahren stark an Popularit\u00e4t gewonnen. Angetrieben von der Entwicklung des Deep Learning und eindrucksvollen Erfolgsgeschichten wie dem Erlernen des Spielens von Atari oder dem L\u00f6sen des Go-Spiels steht eine Familie von Reinforcement Learning Algorithmen im Vordergrund dieses Trends: Deep Reinforcement Learning. Dieser Begriff bezeichnet normalerweise die Kombination von zwei Methoden des maschinellen Lernens, n\u00e4mlich Q-Learning und (m\u00f6glicherweise tiefen) k\u00fcnstlichen neuronalen Netzen, im speziellen bekannt durch die Algorithmen DQN und NFQ. Ohne die F\u00e4higkeiten dieser Kombination herabsetzen zu wollen, ist die Anwendung dieser auf reale Probleme f\u00fcr Praktiker jedoch noch h\u00e4ufig problematisch.<br><br>In dieser Arbeit konzentrieren wir uns daher haupts\u00e4chlich auf zwei Eigenschaften des Deep Reinforcement Learning, die f\u00fcr Anwender besonders wichtig sind, n\u00e4mlich die Stabilit\u00e4t und Dateneffizienz des Deep Reinforcement Learning. Zun\u00e4chst zeigen wir anhand eines Beispiels, dass es bei Deep Reinforcement Learning Methoden zu instabilen Lerndynamiken kommen kann, und schlagen dann einen Algorithmus vor, der die Stabilit\u00e4t sowie die Dateneffizienz auf mehreren Benchmarks verbessert. Zweitens werden wir eine neuartige Anwendung des Reinforcement Learning einf\u00fchren, n\u00e4mlich die Regelung eines biologischen neuronalen Netzwerkes und zeigen, dass sich mit Reinforcement Learning bestimmte Aktivit\u00e4tsmuster dieser Netzwerke kontrollieren lassen. Diese Anwendung wird die Bedeutung stabiler und dateneffizienter Reinforcement Learning Methoden weiter hervorheben",
                "Abstract: Reinforcement Learning is no new discipline in the realm of machine learning, but has seen a surge in popularity and interest from researchers in the last years. Driven by the impact of Deep Learning and impressive success stories such as learning to play Atari on human level or solving the game of Go, one family of Reinforcement Learning methods is in the forefront of said trend: Deep Reinforcement Learning. This term usually refers to the combination of two powerful machine learning methods, namely Q-learning and (possibly deep) artificial neural networks resulting in the popular DQN and NFQ algorithms. Without wanting to belittle the power of said combination, for practitioners there are still many open questions and problems when applying these to a learning task.<br><br>In this thesis we will focus mainly on two properties of Deep Reinforcement Learning that are especially important when dealing with real world applications, namely the stability and sample efficiency of Deep Reinforcement Learning training procedures. First, we will show by example that Deep Reinforcement Learning can suffer from unstable learning dynamics and propose an algorithm that improves stability as well as sample efficiency on several benchmarks. Second, we will introduce a novel application of Reinforcement Learning to a biological system, namely Biological Neural Networks and we will show that it is possible to learn to control certain activity features of these networks. This application will underline the importance of having stable and sample efficient reinforcement learning procedures"
            ],
            "contributor": "Technische Informationsbibliothek (TIB)",
            "creator": "gnd:1106391888",
            "isPartOf": "(collectioncode)GBV-ODiss",
            "issued": "2019",
            "language": "http://id.loc.gov/vocabulary/iso639-1/en",
            "license": "commercial licence",
            "medium": "rda:termList/RDACarrierType/1018",
            "subject": [
                "gnd:4193754-5",
                "gnd:1135597375"
            ],
            "isLike": "doi:10.6094/UNIFR/154753",
            "P60163": "Freiburg im Breisgau"
        }
    ],
    "@id": "urn:x-arq:DefaultGraphNode",
    "@context": {
        "sameAs": "http://www.w3.org/2002/07/owl#sameAs",
        "identifier": "http://purl.org/dc/elements/1.1/identifier",
        "subject": {
            "@id": "http://purl.org/dc/terms/subject",
            "@type": "@id"
        },
        "abstract": "http://purl.org/dc/terms/abstract",
        "creator": {
            "@id": "http://purl.org/dc/terms/creator",
            "@type": "@id"
        },
        "isPartOf": "http://purl.org/dc/terms/isPartOf",
        "description": "http://purl.org/dc/elements/1.1/description",
        "P60163": "http://www.rdaregistry.info/Elements/u/#P60163",
        "title": "http://purl.org/dc/elements/1.1/title",
        "medium": {
            "@id": "http://purl.org/dc/terms/medium",
            "@type": "@id"
        },
        "P1053": "http://iflastandards.info/ns/isbd/elements/P1053",
        "isLike": {
            "@id": "http://umbel.org/umbel#isLike",
            "@type": "@id"
        },
        "issued": "http://purl.org/dc/terms/issued",
        "license": "http://purl.org/dc/terms/license",
        "contributor": "http://purl.org/dc/terms/contributor",
        "language": {
            "@id": "http://purl.org/dc/terms/language",
            "@type": "@id"
        },
        "umbel": "http://umbel.org/umbel#",
        "rdau": "http://www.rdaregistry.info/Elements/u/#",
        "owl": "http://www.w3.org/2002/07/owl#",
        "dcterms": "http://purl.org/dc/terms/",
        "bibo": "http://purl.org/ontology/bibo/",
        "rdam": "http://www.rdaregistry.info/Elements/m/#",
        "gnd": "http://d-nb.info/gnd/",
        "isbd": "http://iflastandards.info/ns/isbd/elements/",
        "rda": "http://rdvocab.info/",
        "doi": "https://doi.org/"
    }
}